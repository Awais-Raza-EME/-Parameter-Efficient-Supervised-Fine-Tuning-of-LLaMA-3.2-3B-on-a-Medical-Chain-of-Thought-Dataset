# ğŸ§  Parameter-Efficient Fine-Tuning of LLaMA 3.2 (3B) on Medical Chain-of-Thought Dataset

This project implements **parameter-efficient supervised fine-tuning** of the [LLaMA 3.2 (3B)] model using **Low-Rank Adaptation (LoRA)** on a specialized **medical Chain-of-Thought (CoT) dataset**. It leverages the [Unsloth](https://github.com/unslothai/unsloth) framework to achieve efficient fine-tuning on consumer-grade GPUs, making powerful medical AI accessible and efficient.

---

## ğŸ“š Overview

- **Model:** LLaMA 3.2 (3B), 4-bit quantized
- **Fine-Tuning Method:** LoRA (Low-Rank Adaptation)
- **Frameworks Used:** Unsloth, Transformers, TRL, WandB
- *
- *Dataset:** Medical CoT dataset from Hugging Face
- **Platform:** Trained on Kaggle GPU (Free Tier)

---

## ğŸ”§ Features

- ğŸ“¦ Efficient 4-bit fine-tuning via LoRA
- ğŸ“ˆ ROUGE-L based evaluation before and after fine-tuning
- ğŸ§ª Structured medical reasoning using Chain-of-Thought prompts
- ğŸ”— Deployment-ready model on Hugging Face
- ğŸ“‹ Fully reproducible training pipeline

---

## ğŸ› ï¸ Installation

```bash
pip install torch wandb numpy pandas datasets transformers trl unsloth rouge-score
```

---

## Project Structure
.
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ fine_tuning_llama_medical.ipynb
â”œâ”€â”€ lora_model/
â”‚   â””â”€â”€ [saved_adapter_files]
â”œâ”€â”€ inference.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

---
